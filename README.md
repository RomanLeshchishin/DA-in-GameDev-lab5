# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #5 выполнил(а):
- Лещишин Роман Александрович
- РИ-210914

Отметка о выполнении заданий (заполняется студентом):
| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## 5. Лабораторная работа. Интеграция экономической системы в проект Unity и обучение ML-Agent
## Цель работы
Обучение ML-Agent на примере экономической системы в Unity.

## Задание 1
Измените параметры файла. yaml-агента и определить какие параметры и 
как влияют на обучение модели.

Обучение модели на стандартных значениях параметров.

Стандартные параметры Economic.yaml:

![Image_1Economic](https://user-images.githubusercontent.com/114608473/208230734-ea46db26-767b-4c0f-8353-817ff01649cc.jpg)

Cцена в Unity
![Image_1TrainingMLA](https://user-images.githubusercontent.com/114608473/208230449-57001425-9950-448a-bb97-028b2aef1a1b.jpg)

Результаты обучения в TensorBoard
![Image_1TensorBoard](https://user-images.githubusercontent.com/114608473/208234228-f9326f0c-d53d-4d40-95f9-902a61e4f3bb.jpg)

По графикам видно, что общее вознаграждение Cumulative Reward растёт, а потеря значения Value Loss, уменьшается.

Какие параметры и как влияют на обучение модели:

Learning_rate

learning_rate - коэффициент скорости обучения. Скорость обучения определяет величину изменений, которую оптимайзер может внести за один раз. Нужно подбирать оптимальное значение: если взять слишком большое - то обучение будет не стабильным - модель будет быстрее обучаться, но и шаги станут больше и модель может застрять на месте; если взять слишком маленькое - модель будет заметно дольше обучаться и точно так же может застревать на месте. Значение следует уменьшать, если обучение нестабильно, а вознаграждение не увеличивается постоянно. Попробовал увеличить параметр до 1.0e-4 и обучить модель:

![Image_1Learning_rate](https://user-images.githubusercontent.com/114608473/208234269-71791bdd-c0ef-4feb-b64c-f8fca396b40e.jpg)

Как видно из графиков, из-за того, что значение увеличилось, сеть "застопарилась" и даже пошла в обратном направлении на промежутке 30000 - 40000 steps.

Уменьшил learning_rate до 1.0e-5, вот результаты:

![Image_1Learning_rate2](https://user-images.githubusercontent.com/114608473/208235190-943e8814-4c89-4698-b36c-82c26cd249ed.jpg)

Вознаграждение увеличивается немного плавнее в отличие от стандартных параметров, остальные графики почти такие же. Стало намного лучше в сравнении с предыдущим значением параметра learning_rate: 1.0e-4, вознаграждение снова растёт, а потеря значения постепенно уменьшается.

Beta

beta - это сила энтропийной регуляризации, которая делает политику агента «более случайной». Это гарантирует, что агенты должным образом исследуют пространство действия во время обучения. Увеличение этого параметра приведёт к выполнению большего количества случайных действий. Параметр должен быть скорректирован таким образом, чтобы Entropy медленно уменьшалась вместе с увеличением вознаграждения. Если энтропия падает слишком медленно, нужно уменьшить beta.

Результаты обучения на стандартных значениях

![Image_1TensorBoardBeta](https://user-images.githubusercontent.com/114608473/208235797-28d068df-10b3-419d-a31b-7e11bfa7f724.jpg)

Изменил на 1.0e-4

![Image_1BetaMain](https://user-images.githubusercontent.com/114608473/208237300-99eccdc9-d1c6-44ab-8d2e-b601e130c92a.jpg)

Вознаграждение за обучение сразу достигло своего пика и перестало расти. При этом Entropy начала медленно уменьшаться.

![Image_1Beta](https://user-images.githubusercontent.com/114608473/208237333-b58dbf9a-8e71-42e3-bc3d-5f45a660e650.jpg)

Epsilon

epsilon влияет на скорость развития политики во время обучения. Соответствует допустимому порогу расхождения между старой и новой политикой при обновлении градиентного спуска. Установка небольшого значения этого параметра приведет к более стабильным обновлениям, но также замедлит процесс обучения.

При увеличении до 0.5 награда оставалась неизменной, число слишком велико. А при маленьком значении, таком как 0.1 - график пришёл в изначальный вид.

Num_epoch

num_epoch - это параметр, отвечающий за количество проходов через буфер опыта при выполнении оптимизации градиентного спуска. При увеличении этого параметра график начинает вести себя нестабильно. Увеличил до 5, вознаграждение стало расти медленее:

![Image_1Num_Epoch](https://user-images.githubusercontent.com/114608473/208241912-7b0d6e66-23f1-43b6-894f-12c2ebf4ea37.jpg)

Lambd

lambd - этот параметр влияет на то, насколько агент полагается на свою текущую оценку стоимости при вычислении обновленной оценки стоимости. Низкие значения соответствуют большему полаганию на собственную оценку.

Уменьшил до 0.7, вознаграждение стало возрастать резкими скачками и только к концу стало увеличиваться монотонно.

![Image_1Lambd](https://user-images.githubusercontent.com/114608473/208243298-c0ec951e-277f-4e15-837d-578a0a6beb82.jpg)

## Задание 2

Cumulative reward - cреднее вознаграждение за эпизод. Имеет небольшие изменения. В лучшем случае, должно последовательно увеличиваться с течением времени.

Episode Length - средняя продолжительность эпизода в среде для агентов.

Policy Loss - этот график определяет величину изменения политики со временем. Должен стремиться вниз, показывая, что политика всё лучше принимает решения.

Value Loss - это средняя потеря функции значения. Будет увеличиваться по мере увеличения вознаграждения, а после становления стабильного награждения должно уменьшаться.

Beta - график, показывающий изменение силы энтропийной регуляризации, которая делает политику агента «более случайной».

Entropy - соответствует тому, насколько случайны решения, показывает величину исследования агента. Должно последовательно уменьшаться с течением обучения.

Learning Rate - соответствует скорости обучения, будет постепенно уменьшаться с течением времени.

Extrinsic Reward - соответствует среднему совокупному вознаграждению, полученному от окружающей среды за эпизод.

Value Estimate - это среднее значение, посещённое всеми состояниями агента. Чтобы отражать увеличение знаний агента, это значение должно расти, а затем стабилизироваться.

## Задание 3
### Какова роль параметра Lr? Ответьте на вопрос, приведите пример выполнения кода, который подтверждает ваш ответ. В качестве эксперимента можете изменить значение параметра.

- Перечисленные в этом туториале действия могут быть выполнены запуском на исполнение скрипт-файла, доступного [в репозитории](https://github.com/Den1sovDm1triy/hfss-scripting/blob/main/ScreatingSphereInAEDT.py).
- Для запуска скрипт-файла откройте Ansys Electronics Desktop. Перейдите во вкладку [Automation] - [Run Script] - [Выберите файл с именем ScreatingSphereInAEDT.py из репозитория].

```py

import ScriptEnv
ScriptEnv.Initialize("Ansoft.ElectronicsDesktop")
oDesktop.RestoreWindow()
oProject = oDesktop.NewProject()
oProject.Rename("C:/Users/denisov.dv/Documents/Ansoft/SphereDIffraction.aedt", True)
oProject.InsertDesign("HFSS", "HFSSDesign1", "HFSS Terminal Network", "")
oDesign = oProject.SetActiveDesign("HFSSDesign1")
oEditor = oDesign.SetActiveEditor("3D Modeler")
oEditor.CreateSphere(
	[
		"NAME:SphereParameters",
		"XCenter:="		, "0mm",
		"YCenter:="		, "0mm",
		"ZCenter:="		, "0mm",
		"Radius:="		, "1.0770329614269mm"
	], 
)

```

## Выводы

Абзац умных слов о том, что было сделано и что было узнано.

| Plugin | README |
| ------ | ------ |
| Dropbox | [plugins/dropbox/README.md][PlDb] |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |
| OneDrive | [plugins/onedrive/README.md][PlOd] |
| Medium | [plugins/medium/README.md][PlMe] |
| Google Analytics | [plugins/googleanalytics/README.md][PlGa] |

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
